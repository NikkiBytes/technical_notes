{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Build `x-bte` Schema Validation Report  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controller.smartapi import SmartAPI\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from jsonschema.exceptions import ValidationError\n",
    "import datetime\n",
    "import yaml\n",
    "from jsonschema import validate\n",
    "import jsonschema.exceptions\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Validate with test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_bte = \"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/x-bte_schema_v3.json\"\n",
    "# Load the JSON file\n",
    "with open(schema_bte, 'r') as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Print the JSON\n",
    "#print(json.dumps(schema, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test documents \n",
    "- \"Multiomics ClinicalTrials KP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(test_doc):\n",
    "    # Load the JSON document\n",
    "    with open(test_doc, 'r') as file:\n",
    "        document = json.load(file)\n",
    "    # Print the JSON\n",
    "    # print(json.dumps(document, indent=4))\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_kp_doc = load_json(\"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/x-bte_test_doc.json\")\n",
    "quickgo_doc = load_json(\"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/quickgo_doc.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_json(document, schema):\n",
    "    try:\n",
    "        validate(instance=document, schema=schema)\n",
    "        print(\"The document is valid.\")\n",
    "    except ValidationError as e:\n",
    "        print(\"The document is not valid. See below for more details.\")\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate with API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is valid.\n"
     ]
    }
   ],
   "source": [
    "validate_json(quickgo_doc, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Report  \n",
    "**Gather x-bte related documents only**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x-translator.team` = Service Provider -- has x-bte annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = {\"type\": \"term\", \"body\": {\"tags.name\": \"translator\"}}\n",
    "doc_ct = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write report for original schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_name=\"x-bte_schemav3_report_\"+datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")+\".txt\"\n",
    "\n",
    "pass_count = 0\n",
    "fail_count = 0\n",
    "doc_ct = 0\n",
    "fail_ids = []\n",
    "fail_details = []\n",
    "missing_properties = defaultdict(int)\n",
    "error_categories = defaultdict(int)\n",
    "error_summary = defaultdict(int)\n",
    "\n",
    "# Function to validate JSON against a schema\n",
    "def validate_json(data_doc, schema):\n",
    "    global pass_count, fail_count\n",
    "    try:\n",
    "        # Load the YAML formatted string into a Python dictionary\n",
    "        source_data = yaml.safe_load(data_doc)\n",
    "\n",
    "        # Validate the entry against the schema\n",
    "        validate(instance=source_data, schema=schema)\n",
    "        pass_count += 1  # Increment pass counter\n",
    "\n",
    "    except yaml.YAMLError as ye:\n",
    "        fail_count += 1  # Increment fail counter\n",
    "        fail_ids.append(smartapi._id)  # Append the failed entry's ID\n",
    "        fail_details.append(f\"YAML Error for ID {smartapi._id}: {ye}\")\n",
    "    except jsonschema.exceptions.ValidationError as ve:\n",
    "        fail_count += 1  # Increment fail counter\n",
    "        fail_ids.append(smartapi._id)  # Append the failed entry's ID\n",
    "        error_message = f\"Schema Validation Error for ID {smartapi._id}: {ve}\"\n",
    "        fail_details.append(error_message)\n",
    "        # Check if error is about a missing required property\n",
    "        if 'is a required property' in str(ve):\n",
    "            missing_prop = str(ve).split(\"'\")[1]  # Extract the missing property name\n",
    "            missing_properties[missing_prop] += 1\n",
    "        # Categorize and count other types of validation errors\n",
    "        if 'enum' in str(ve):\n",
    "            error_categories['Enum Constraint Violations'] += 1\n",
    "        elif 'additionalProperties' in str(ve):\n",
    "            error_categories['Unexpected Properties'] += 1\n",
    "        else:\n",
    "            error_categories['Other Errors'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary: 127 passed, 2 failed.\n",
      "Summary Count of Unique Error IDs: 2\n",
      "Failed Validation IDs: 671b45c0301c8624abbd26ae78449ca2, cc857d5b7c8b7609b5bbb38ff990bfff\n"
     ]
    }
   ],
   "source": [
    "# Open a file to write the report\n",
    "with open(report_name, 'w') as report_file:\n",
    "    # Traverse all SmartAPI entries with the given query\n",
    "    for smartapi in SmartAPI.get_all(1000, query_data=query_data):\n",
    "        doc_ct += 1\n",
    "        # Decode the raw byte data to a string\n",
    "        data_doc = smartapi.raw.decode('utf-8')\n",
    "        validate_json(data_doc, schema)\n",
    "\n",
    "    total_entries = pass_count + fail_count\n",
    "    percent_passed = (pass_count / total_entries * 100) if total_entries else 0\n",
    "    percent_failed = (fail_count / total_entries * 100) if total_entries else 0\n",
    "    unique_error_ids = len(set(fail_ids))\n",
    "\n",
    "    # Write the top summary statistics\n",
    "    report_file.write(f\"Validation Report Generated on {datetime.datetime.now()}\\n\")\n",
    "    report_file.write(\"-------------------------------------------------\\n\")\n",
    "    report_file.write(f\"Total Entries Processed: {total_entries}\\n\")\n",
    "    report_file.write(f\"Total Passed: {pass_count} ({percent_passed:.2f}%)\\n\")\n",
    "    report_file.write(f\"Total Failed: {fail_count} ({percent_failed:.2f}%)\\n\\n\")\n",
    "\n",
    "    # Error Type Summary\n",
    "    if error_summary:\n",
    "        report_file.write(\"Error Type Summary:\\n\")\n",
    "        for error_type, count in error_summary.items():\n",
    "            report_file.write(f\"{error_type}: {count}\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Validation Error Categories\n",
    "    if error_categories:\n",
    "        report_file.write(\"Validation Error Categories Summary:\\n\")\n",
    "        for category, count in error_categories.items():\n",
    "            report_file.write(f\"{category}: {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Missing Required Properties\n",
    "    if missing_properties:\n",
    "        report_file.write(\"Missing Required Property Summary:\\n\")\n",
    "        for prop, count in missing_properties.items():\n",
    "            report_file.write(f\"Missing '{prop}': {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Most Common Errors\n",
    "    if error_categories:\n",
    "        most_common_error = max(error_categories, key=error_categories.get)\n",
    "        report_file.write(f\"Most Common Error Type: {most_common_error} ({error_categories[most_common_error]} occurrences)\\n\\n\")\n",
    "\n",
    "    # Summary of Most Common Missing Properties\n",
    "    if missing_properties:\n",
    "        most_common_missing_prop = max(missing_properties, key=missing_properties.get)\n",
    "        report_file.write(f\"Most Common Missing Property: '{most_common_missing_prop}' ({missing_properties[most_common_missing_prop]} times)\\n\\n\")\n",
    "\n",
    "    # Summary List of Error IDs\n",
    "    if fail_ids:\n",
    "        report_file.write(\"Summary List of Error IDs:\\n\")\n",
    "        report_file.write(\", \".join(fail_ids) + \"\\n\\n\")\n",
    "\n",
    "    # Summary Count of Unique Error IDs\n",
    "    if unique_error_ids > 0:\n",
    "        report_file.write(f\"Summary Count of Unique Error IDs: {unique_error_ids}\\n\\n\")\n",
    "\n",
    "    # Detailed Error Reports\n",
    "    if fail_details:\n",
    "        report_file.write(\"Detailed Error Reports:\\n\")\n",
    "        report_file.write(\"-------------------------------------------------\\n\")\n",
    "        for detail in fail_details:\n",
    "            report_file.write(detail + \"\\n\")\n",
    "            report_file.write(\"-------------------------------------------------\\n\")\n",
    "        report_file.write(\"\\n\")\n",
    "\n",
    "# Optionally, you can also print the summary to the console\n",
    "print(f\"Validation Summary: {pass_count} passed, {fail_count} failed.\")\n",
    "if unique_error_ids > 0:\n",
    "    print(f\"Summary Count of Unique Error IDs: {unique_error_ids}\")\n",
    "if fail_ids:\n",
    "    print(f\"Failed Validation IDs: {', '.join(fail_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema - Polished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_bte= \"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/x-bte_schema_final.json\"\n",
    "with open(schema_bte, 'r') as f:\n",
    "    schema = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = {\"type\": \"term\", \"body\": {\"info.x-translator.team\": \"Service Provider\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate JSON against a schema\n",
    "def validate_json(data_doc, schema):\n",
    "    global pass_count, fail_count\n",
    "    try:\n",
    "        # Load the YAML formatted string into a Python dictionary\n",
    "        source_data = yaml.safe_load(data_doc)\n",
    "\n",
    "        # Validate the entry against the schema\n",
    "        validate(instance=source_data, schema=schema)\n",
    "        pass_count += 1  # Increment pass counter\n",
    "\n",
    "    except yaml.YAMLError as ye:\n",
    "        fail_count += 1  # Increment fail counter\n",
    "        fail_ids.append(smartapi._id)  # Append the failed entry's ID\n",
    "        fail_details.append(f\"YAML Error for ID {smartapi._id}: {ye}\")\n",
    "    except jsonschema.exceptions.ValidationError as ve:\n",
    "        fail_count += 1  # Increment fail counter\n",
    "        fail_ids.append(smartapi._id)  # Append the failed entry's ID\n",
    "        error_message = f\"Schema Validation Error for ID {smartapi._id}: {ve}\"\n",
    "        fail_details.append(error_message)\n",
    "        # Check if error is about a missing required property\n",
    "        if 'is a required property' in str(ve):\n",
    "            missing_prop = str(ve).split(\"'\")[1]  # Extract the missing property name\n",
    "            missing_properties[missing_prop] += 1\n",
    "        # Categorize and count other types of validation errors\n",
    "        if 'enum' in str(ve):\n",
    "            error_categories['Enum Constraint Violations'] += 1\n",
    "        elif 'additionalProperties' in str(ve):\n",
    "            error_categories['Unexpected Properties'] += 1\n",
    "        else:\n",
    "            error_categories['Other Errors'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Open a file to write the report\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(report_name, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m report_file, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx-bte_errors.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m error_file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Traverse all SmartAPI entries with the given query\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mfor\u001b[39;00m smartapi \u001b[39min\u001b[39;00m SmartAPI\u001b[39m.\u001b[39mget_all(\u001b[39m1000\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         doc_ct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/Report-X-BTE_schema.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             \u001b[39m# Decode the raw byte data to a string\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/controller/base.py:190\u001b[0m, in \u001b[0;36mAbstractWebEntity.get_all\u001b[0;34m(cls, size, from_, query_data)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mfor\u001b[39;00m hit \u001b[39min\u001b[39;00m search:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:  \u001b[39m# unlikely but possible\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         doc \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget(hit\u001b[39m.\u001b[39;49mmeta\u001b[39m.\u001b[39;49mid)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m ESNotFoundError:\n\u001b[1;32m    192\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# inconsistent\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/controller/smartapi.py:118\u001b[0m, in \u001b[0;36mSmartAPI.get\u001b[0;34m(cls, _id)\u001b[0m\n\u001b[1;32m    116\u001b[0m obj\u001b[39m.\u001b[39musername \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39musername\n\u001b[1;32m    117\u001b[0m obj\u001b[39m.\u001b[39mslug \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mslug\n\u001b[0;32m--> 118\u001b[0m obj\u001b[39m.\u001b[39;49mraw \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mdecompress(obj\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39m_raw)\n\u001b[1;32m    120\u001b[0m obj\u001b[39m.\u001b[39mdate_created \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mdate_created\n\u001b[1;32m    121\u001b[0m obj\u001b[39m.\u001b[39mlast_updated \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mlast_updated\n",
      "File \u001b[0;32m~/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/controller/smartapi.py:244\u001b[0m, in \u001b[0;36mSmartAPI.raw\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m ControllerError(\u001b[39m\"\u001b[39m\u001b[39mEmpty value.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mto_dict(value)\n\u001b[1;32m    245\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m ControllerError(\u001b[39mstr\u001b[39m(err)) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/utils/decoder.py:68\u001b[0m, in \u001b[0;36mto_dict\u001b[0;34m(stream, ext, ctype)\u001b[0m\n\u001b[1;32m     65\u001b[0m         stream \u001b[39m=\u001b[39m stream[\u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mexport default \u001b[39m\u001b[39m\"\u001b[39m) :]\n\u001b[1;32m     67\u001b[0m \u001b[39m# brute force\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m to_yaml(stream)\n",
      "File \u001b[0;32m~/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/utils/decoder.py:17\u001b[0m, in \u001b[0;36mto_yaml\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_yaml\u001b[39m(stream):\n\u001b[1;32m     16\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m         data \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39;49mload(stream, Loader\u001b[39m=\u001b[39;49myaml\u001b[39m.\u001b[39;49mSafeLoader)\n\u001b[1;32m     18\u001b[0m     \u001b[39mexcept\u001b[39;00m (yaml\u001b[39m.\u001b[39mscanner\u001b[39m.\u001b[39mScannerError, yaml\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mParserError) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     19\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mstr\u001b[39m(err)) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[39m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m loader\u001b[39m.\u001b[39;49mget_single_data()\n\u001b[1;32m     82\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[39m.\u001b[39mdispose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/constructor.py:49\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_single_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[39m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_single_node()\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:36\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m document \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(StreamEndEvent):\n\u001b[0;32m---> 36\u001b[0m     document \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_document()\n\u001b[1;32m     38\u001b[0m \u001b[39m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(StreamEndEvent):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:55\u001b[0m, in \u001b[0;36mComposer.compose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_event()\n\u001b[1;32m     54\u001b[0m \u001b[39m# Compose the root node.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_event()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_mapping_node(anchor)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_node(node, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, item_key)\n\u001b[1;32m    134\u001b[0m \u001b[39m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_mapping_node(anchor)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_node(node, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, item_key)\n\u001b[1;32m    134\u001b[0m \u001b[39m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_mapping_node(anchor)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_node(node, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, item_key)\n\u001b[1;32m    134\u001b[0m \u001b[39m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:82\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     80\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_scalar_node(anchor)\n\u001b[1;32m     81\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(SequenceStartEvent):\n\u001b[0;32m---> 82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[1;32m     84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_mapping_node(anchor)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:111\u001b[0m, in \u001b[0;36mComposer.compose_sequence_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    109\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(SequenceEndEvent):\n\u001b[0;32m--> 111\u001b[0m     node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, index))\n\u001b[1;32m    112\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    113\u001b[0m end_event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_event()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_mapping_node(anchor)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_node(node, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, item_key)\n\u001b[1;32m    134\u001b[0m \u001b[39m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:82\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     80\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_scalar_node(anchor)\n\u001b[1;32m     81\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(SequenceStartEvent):\n\u001b[0;32m---> 82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[1;32m     84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_mapping_node(anchor)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:111\u001b[0m, in \u001b[0;36mComposer.compose_sequence_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    109\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(SequenceEndEvent):\n\u001b[0;32m--> 111\u001b[0m     node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, index))\n\u001b[1;32m    112\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    113\u001b[0m end_event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_event()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_mapping_node(anchor)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_node(node, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompose_node(node, item_key)\n\u001b[1;32m    134\u001b[0m \u001b[39m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/composer.py:79\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[39mraise\u001b[39;00m ComposerError(\u001b[39m\"\u001b[39m\u001b[39mfound duplicate anchor \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m; first occurrence\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[39m%\u001b[39m anchor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchors[anchor]\u001b[39m.\u001b[39mstart_mark,\n\u001b[1;32m     77\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msecond occurrence\u001b[39m\u001b[39m\"\u001b[39m, event\u001b[39m.\u001b[39mstart_mark)\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdescend_resolver(parent, index)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_event(ScalarEvent):\n\u001b[1;32m     80\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompose_scalar_node(anchor)\n\u001b[1;32m     81\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_event(SequenceStartEvent):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/yaml/parser.py:100\u001b[0m, in \u001b[0;36mParser.check_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate()\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_event \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m choices:\n\u001b[1;32m    101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m choices:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# query_data = {\n",
    "#     \"type\": \"match\",\n",
    "#     \"body\": {\n",
    "#         \"info.x-translator.team\": \"Service Provider\"\n",
    "#     }\n",
    "# }\n",
    "report_name=\"report_x-bte_schema_val_CURRENT\"+datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")+\".txt\"\n",
    "\n",
    "pass_count = 0\n",
    "fail_count = 0\n",
    "doc_ct = 0\n",
    "fail_ids = []\n",
    "fail_details = []\n",
    "missing_properties = defaultdict(int)\n",
    "error_categories = defaultdict(int)\n",
    "error_summary = defaultdict(int)\n",
    "\n",
    "schema_bte= \"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/smartapi_x-bte_schema.json\"\n",
    "with open(schema_bte, 'r') as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Open a file to write the report\n",
    "with open(report_name, 'w') as report_file, open(\"x-bte_errors.txt\", 'w') as error_file:\n",
    "    # Traverse all SmartAPI entries with the given query\n",
    "    for smartapi in SmartAPI.get_all(1000):\n",
    "        doc_ct += 1\n",
    "        try:\n",
    "            # Decode the raw byte data to a string\n",
    "            data_doc = smartapi.raw.decode('utf-8')\n",
    "            # Check if \"x-bte-kgs-operations\" exists in the document\n",
    "            # if \"x-bte-kgs-operations:\" in data_doc:\n",
    "            validate_json(data_doc, schema)\n",
    "            # else: \n",
    "            #     continue\n",
    "        except Exception as e:\n",
    "            error_file.write(f\"\\n----------\\n{e}\\n{data_doc}\\n----------\\n\")\n",
    "            \n",
    "    total_entries = pass_count + fail_count\n",
    "    percent_passed = (pass_count / total_entries * 100) if total_entries else 0\n",
    "    percent_failed = (fail_count / total_entries * 100) if total_entries else 0\n",
    "    unique_error_ids = len(set(fail_ids))\n",
    "\n",
    "    # Write the top summary statistics\n",
    "    report_file.write(f\"Validation Report Generated on {datetime.datetime.now()}\\n\")\n",
    "    report_file.write(\"-------------------------------------------------\\n\")\n",
    "    report_file.write(f\"Total Entries Processed: {total_entries}\\n\")\n",
    "    report_file.write(f\"Total Passed: {pass_count} ({percent_passed:.2f}%)\\n\")\n",
    "    report_file.write(f\"Total Failed: {fail_count} ({percent_failed:.2f}%)\\n\\n\")\n",
    "\n",
    "    # Error Type Summary\n",
    "    if error_summary:\n",
    "        report_file.write(\"Error Type Summary:\\n\")\n",
    "        for error_type, count in error_summary.items():\n",
    "            report_file.write(f\"{error_type}: {count}\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Validation Error Categories\n",
    "    if error_categories:\n",
    "        report_file.write(\"Validation Error Categories Summary:\\n\")\n",
    "        for category, count in error_categories.items():\n",
    "            report_file.write(f\"{category}: {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Missing Required Properties\n",
    "    if missing_properties:\n",
    "        report_file.write(\"Missing Required Property Summary:\\n\")\n",
    "        for prop, count in missing_properties.items():\n",
    "            report_file.write(f\"Missing '{prop}': {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Most Common Errors\n",
    "    if error_categories:\n",
    "        most_common_error = max(error_categories, key=error_categories.get)\n",
    "        report_file.write(f\"Most Common Error Type: {most_common_error} ({error_categories[most_common_error]} occurrences)\\n\\n\")\n",
    "\n",
    "    # Summary of Most Common Missing Properties\n",
    "    if missing_properties:\n",
    "        most_common_missing_prop = max(missing_properties, key=missing_properties.get)\n",
    "        report_file.write(f\"Most Common Missing Property: '{most_common_missing_prop}' ({missing_properties[most_common_missing_prop]} times)\\n\\n\")\n",
    "\n",
    "    # Summary List of Error IDs\n",
    "    if fail_ids:\n",
    "        report_file.write(\"Summary List of Error IDs:\\n\")\n",
    "        report_file.write(\", \".join(fail_ids) + \"\\n\\n\")\n",
    "\n",
    "    # Summary Count of Unique Error IDs\n",
    "    if unique_error_ids > 0:\n",
    "        report_file.write(f\"Summary Count of Unique Error IDs: {unique_error_ids}\\n\\n\")\n",
    "\n",
    "    # Detailed Error Reports\n",
    "    if fail_details:\n",
    "        report_file.write(\"Detailed Error Reports:\\n\")\n",
    "        report_file.write(\"-------------------------------------------------\\n\")\n",
    "        for detail in fail_details:\n",
    "            report_file.write(detail + \"\\n\")\n",
    "            report_file.write(\"-------------------------------------------------\\n\")\n",
    "        report_file.write(\"\\n\")\n",
    "\n",
    "# Optionally, you can also print the summary to the console\n",
    "print(f\"Validation Summary: {pass_count} passed, {fail_count} failed.\")\n",
    "if unique_error_ids > 0:\n",
    "    print(f\"Summary Count of Unique Error IDs: {unique_error_ids}\")\n",
    "if fail_ids:\n",
    "    print(f\"Failed Validation IDs: {', '.join(fail_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary: 268 passed, 3 failed.\n",
      "Summary Count of Unique Error IDs: 3\n",
      "Failed Validation IDs: 671b45c0301c8624abbd26ae78449ca2, cc857d5b7c8b7609b5bbb38ff990bfff, 1f02d8b032f0732f41711fd5c637567f\n"
     ]
    }
   ],
   "source": [
    "# query_data = {\n",
    "#     \"type\": \"match\",\n",
    "#     \"body\": {\n",
    "#         \"info.x-translator.team\": \"Service Provider\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "schema_bte= \"/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/smartapi_x-bte_schema.json\"\n",
    "with open(schema_bte, 'r') as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "report_name=\"report_x-bte_schema_val_CURRENT\"+datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")+\".txt\"\n",
    "\n",
    "pass_count = 0\n",
    "fail_count = 0\n",
    "doc_ct = 0\n",
    "fail_ids = []\n",
    "fail_details = []\n",
    "missing_properties = defaultdict(int)\n",
    "error_categories = defaultdict(int)\n",
    "error_summary = defaultdict(int)\n",
    "\n",
    "# Open a file to write the report\n",
    "with open(report_name, 'w') as report_file, open(\"x-bte_errors.txt\", 'w') as error_file:\n",
    "    # Traverse all SmartAPI entries with the given query\n",
    "    for smartapi in SmartAPI.get_all(1000):\n",
    "        doc_ct += 1\n",
    "        try:\n",
    "            # Decode the raw byte data to a string\n",
    "            data_doc = smartapi.raw.decode('utf-8')\n",
    "            # Check if \"x-bte-kgs-operations\" exists in the document\n",
    "            # if \"x-bte-kgs-operations:\" in data_doc:\n",
    "            validate_json(data_doc, schema)\n",
    "            # else: \n",
    "            #     continue\n",
    "        except Exception as e:\n",
    "            error_file.write(f\"\\n----------\\n{e}\\n{data_doc}\\n----------\\n\")\n",
    "            \n",
    "    total_entries = pass_count + fail_count\n",
    "    percent_passed = (pass_count / total_entries * 100) if total_entries else 0\n",
    "    percent_failed = (fail_count / total_entries * 100) if total_entries else 0\n",
    "    unique_error_ids = len(set(fail_ids))\n",
    "\n",
    "    # Write the top summary statistics\n",
    "    report_file.write(f\"Validation Report Generated on {datetime.datetime.now()}\\n\")\n",
    "    report_file.write(\"-------------------------------------------------\\n\")\n",
    "    report_file.write(f\"Total Entries Processed: {total_entries}\\n\")\n",
    "    report_file.write(f\"Total Passed: {pass_count} ({percent_passed:.2f}%)\\n\")\n",
    "    report_file.write(f\"Total Failed: {fail_count} ({percent_failed:.2f}%)\\n\\n\")\n",
    "\n",
    "    # Error Type Summary\n",
    "    if error_summary:\n",
    "        report_file.write(\"Error Type Summary:\\n\")\n",
    "        for error_type, count in error_summary.items():\n",
    "            report_file.write(f\"{error_type}: {count}\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Validation Error Categories\n",
    "    if error_categories:\n",
    "        report_file.write(\"Validation Error Categories Summary:\\n\")\n",
    "        for category, count in error_categories.items():\n",
    "            report_file.write(f\"{category}: {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Missing Required Properties\n",
    "    if missing_properties:\n",
    "        report_file.write(\"Missing Required Property Summary:\\n\")\n",
    "        for prop, count in missing_properties.items():\n",
    "            report_file.write(f\"Missing '{prop}': {count} times\\n\")\n",
    "\n",
    "    report_file.write(\"\\n\")\n",
    "\n",
    "    # Summary of Most Common Errors\n",
    "    if error_categories:\n",
    "        most_common_error = max(error_categories, key=error_categories.get)\n",
    "        report_file.write(f\"Most Common Error Type: {most_common_error} ({error_categories[most_common_error]} occurrences)\\n\\n\")\n",
    "\n",
    "    # Summary of Most Common Missing Properties\n",
    "    if missing_properties:\n",
    "        most_common_missing_prop = max(missing_properties, key=missing_properties.get)\n",
    "        report_file.write(f\"Most Common Missing Property: '{most_common_missing_prop}' ({missing_properties[most_common_missing_prop]} times)\\n\\n\")\n",
    "\n",
    "    # Summary List of Error IDs\n",
    "    if fail_ids:\n",
    "        report_file.write(\"Summary List of Error IDs:\\n\")\n",
    "        report_file.write(\", \".join(fail_ids) + \"\\n\\n\")\n",
    "\n",
    "    # Summary Count of Unique Error IDs\n",
    "    if unique_error_ids > 0:\n",
    "        report_file.write(f\"Summary Count of Unique Error IDs: {unique_error_ids}\\n\\n\")\n",
    "\n",
    "    # Detailed Error Reports\n",
    "    if fail_details:\n",
    "        report_file.write(\"Detailed Error Reports:\\n\")\n",
    "        report_file.write(\"-------------------------------------------------\\n\")\n",
    "        for detail in fail_details:\n",
    "            report_file.write(detail + \"\\n\")\n",
    "            report_file.write(\"-------------------------------------------------\\n\")\n",
    "        report_file.write(\"\\n\")\n",
    "\n",
    "# Optionally, you can also print the summary to the console\n",
    "print(f\"Validation Summary: {pass_count} passed, {fail_count} failed.\")\n",
    "if unique_error_ids > 0:\n",
    "    print(f\"Summary Count of Unique Error IDs: {unique_error_ids}\")\n",
    "if fail_ids:\n",
    "    print(f\"Failed Validation IDs: {', '.join(fail_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03283cc2b21c077be6794e1704b1d230\n",
      "'x-bte-kgs-operations' found in document 03283cc2b21c077be6794e1704b1d230\n"
     ]
    }
   ],
   "source": [
    "for smartapi in SmartAPI.get_all(1000, query_data=query_data):\n",
    "    print(smartapi._id)\n",
    "    # print(smartapi.raw.de    # Decode the raw byte data to a string\n",
    "    data_doc = smartapi.raw.decode('utf-8')\n",
    "    # Check if \"x-bte-kgs-operations\" exists in the document\n",
    "    if \"x-bte-kgs-operations\" in data_doc:\n",
    "        print(f\"'x-bte-kgs-operations' found in document {smartapi._id}\")\n",
    "    else:\n",
    "        print(f\"'x-bte-kgs-operations' not found in document {smartapi._id}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "report='/Users/nacosta/Documents/smartAPI/WORKING_BRANCH/add-validation/smartAPI/src/x-bte_schemav3_report_20240726114935.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [{'qInput': 'MONDO:0009346', 'oneOutput': 'MONDO:0021568'}] is not of type 'object'\n",
      "IDs: 671b45c0301c8624abbd26ae78449ca2: [{'qInput': 'MONDO:0009346', 'oneOutput': 'MONDO, 671b45c0301c8624abbd26ae78449ca2: [{'qInput': 'MONDO:0009346', 'oneOutput': 'MONDO\n",
      "Details: \n",
      "Failed validating 'type' in schema['properties']['components']['properties']['x-bte-kgs-operations']['patternProperties']['^[A-Za-z0-9_-]+$']['items']['properties']['response_mapping']['patternProperties']['^[A-Za-z0-9_:-]+$']:\n",
      "    {'type': 'object'}\n",
      "\n",
      "On instance['components']['x-bte-kgs-operations']['disease_arises_from_feature'][0]['response_mapping']['testExamples']:\n",
      "    [{'oneOutput': 'MONDO:0021568', 'qInput': 'MONDO:0009346'}]\n",
      "\n",
      "\n",
      "Error: 'supportBatch' is a required property\n",
      "IDs: cc857d5b7c8b7609b5bbb38ff990bfff, cc857d5b7c8b7609b5bbb38ff990bfff\n",
      "Details: \n",
      "Failed validating 'required' in schema['properties']['components']['properties']['x-bte-kgs-operations']['patternProperties']['^[A-Za-z0-9_-]+$']['items']:\n",
      "    {'properties': {'agent_type': {'type': 'string'},\n",
      "                    'inputs': {'items': {'properties': {'id': {'type': 'string'},\n",
      "                                                        'semantic': {'type': 'string'}},\n",
      "                                         'required': ['id', 'semantic'],\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Open the file\n",
    "with open(report, 'r') as file:\n",
    "    # Initialize a flag for the \"Detailed Error Reports\" section\n",
    "    detailed_errors = False\n",
    "    # Initialize a dictionary to store the error messages and IDs\n",
    "    error_dict = {}\n",
    "    # Initialize a dictionary to store the detailed error information\n",
    "    error_info_dict = {}\n",
    "\n",
    "    # Loop through each line in the file\n",
    "    for line in file:\n",
    "        # If the line contains \"Detailed Error Reports\", set the flag to True\n",
    "        if \"Detailed Error Reports\" in line:\n",
    "            detailed_errors = True\n",
    "\n",
    "        # If the flag is True, process the line\n",
    "        if detailed_errors:\n",
    "            # If the line contains \"Schema Validation Error for ID\", extract the ID and error message\n",
    "            if \"Schema Validation Error for ID\" in line:\n",
    "                id = re.search(r\"for ID (.*):\", line).group(1)\n",
    "                error = re.search(r\": (.*)\", line).group(1)\n",
    "                # If the error message is already in the dictionary, append the ID to the list of IDs\n",
    "                if error in error_dict:\n",
    "                    error_dict[error].append(id)\n",
    "                # If the error message is not in the dictionary, add it with the ID as the first item in the list of IDs\n",
    "                else:\n",
    "                    error_dict[error] = [id]\n",
    "                    # Store the next 6 lines as the detailed error information\n",
    "                    error_info_dict[error] = ''.join([next(file) for _ in range(6)])\n",
    "\n",
    "# Print out the error messages, IDs, and detailed error information\n",
    "for error, ids in error_dict.items():\n",
    "    print(f\"Error: {error}\")\n",
    "    print(f\"IDs: {', '.join(ids)}\")\n",
    "    print(f\"Details: {error_info_dict[error]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
